<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Green Milk Tea Shop Classifier</title>
  <link rel="stylesheet" href="style.css?v=8">
</head>
<body>

  <main class="section">
    <div class="container">

      <div class="project-row">

        <figure class="project-figure">
          <img
            src="./images/esp32-setup.png"
            alt="ESP32 setup"
            class="project-img"
          >
        </figure>

        <div class="project-text">
          
          <h1 class="project-title">Green Milk Tea Shop Classifier</h1>
          <p class="muted">
            ESP32-based system integrating five sensors: color, total dissolved solids (TDS),
            turbidity, temperature, and pH. The apparatus was used to collect sensor data from
            green milk tea samples across five different retail stores.
          </p>

          <h2 class="project-section">Objective</h2>
          <p class="muted">
            The objective of this project was to use a sensor dataâ€“driven machine learning
            approach to distinguish between green milk tea samples using the k-nearest neighbors (kNN) algorithm.
          </p>

          <h2 class="project-section">Methods</h2>
          <p class="muted">
             To minimize variability, all green milk tea samples were ordered with zero sugar and no ice. 
             Sensors were not formally calibrated. However, all sensor measurements were recorded using the same apparatus and consistent conditions. 
             With the exception of total dissolved solids (TDS) and temperature, all sensor readings were taken as raw values. 
             Measurements were recorded every 10 seconds over a 500-second period, with the first 200 seconds excluded from the analysis to maintain stabilized readings.
             Although readings from the F1-F8 spectral channels, which span approximately 415-680nm, were sample-specific and valuable, they were excluded due to the risk of overfitting the kNN model. 
             Temperature readings were also excluded due to low variability between samples and treated as a noise factor. 

             A kNN classifier was selected because it does not require explicit model training and performs well with small datasets. 
             All remaining features were normalized to ensure equal weighting during kNN classification.             
          </p>

          <h2 class="project-section">Results</h2>
          <p class="muted">
             Model performance was evaluated using a classification report summarizing preciision, recall, and F1-score across the five locations.
          </p>

          <figure class="project-figure">
          <img
            src="./images/classification-report.png"
            class="project-img"
          >
        </figure>



          
        </div>

      </div>

    </div>
  </main>

</body>
</html>
